| **Title** | **Category** | **Repo** | **Paper** | **Key words** |
|-----------|:-----------:|:--------:|:---------:|:-----------:|
| Online Matching with Stochastic Rewards: Provable Better Bound via Adversarial Reinforcement Learning | **Oral** | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/zhang24bf.html) | - |
| Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models | **Oral** | [![Github](https://img.shields.io/github/stars/chs20/RobustVLM)](https://github.com/chs20/RobustVLM) | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/schlarmann24a.html) | - |
| Towards Optimal Adversarial Robust Q-learning with Bellman Infinity-error | **Oral** | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/li24cl.html) | - |
| Low-Cost High-Power Membership Inference Attacks | **Oral** | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/zarifzadeh24a.html) | - |
| The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright BreachesWithout Adjusting Finetuning Pipeline | **Oral** | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/wang24bm.html) | - |
| Private Truly-Everlasting Robust-Prediction | **Oral** | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/stemmer24a.html) | - |
| Position: On the Societal Impact of Open Foundation Models | **Oral** | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/kapoor24a.html) | - |
| Trained Random Forests Completely Reveal your Dataset | **Oral** | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/ferry24a.html) | - |
| Stealing part of a production language model | **Oral** | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/carlini24a.html) | - |
| AI Control: Improving Safety Despite Intentional Subversion | **Oral** | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/greenblatt24a.html) | - |
| Bridging Data Gaps in Diffusion Models with Adversarial Noise-Based Transfer Learning | **Spotlight** | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/wang24ap.html) | - |
| A Theoretical Analysis of Backdoor Poisoning Attacks in Convolutional Neural Networks | **Spotlight** | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/li24at.html) | - |
| Improving Interpretation Faithfulness for Vision Transformers | **Spotlight** | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/hu24k.html) | - |
| Fool Your (Vision and) Language Model with Embarrassingly Simple Permutations | Poster | [![Github](https://img.shields.io/github/stars/ys-zong/FoolyourVLLMs)](https://github.com/ys-zong/FoolyourVLLMs) | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/zong24b.html) | - |
| Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models | Poster | [![Github](https://img.shields.io/github/stars/ys-zong/VLGuard)](https://github.com/ys-zong/VLGuard) | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/zong24a.html) | - |
| RAUCA: A Novel Physical Adversarial Attack on Vehicle Detectors via Robust and Accurate Camouflage Generation | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/zhou24n.html) | - |
| Attack-free Evaluating and Enhancing Adversarial Robustness on Categorical Data | Poster | [![Github](https://img.shields.io/github/stars/YujunZhou/IGSG)](https://github.com/YujunZhou/IGSG) | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/zhou24i.html) | - |
| Towards Efficient Training and Evaluation of Robust Models against $ùëô_0$ Bounded Adversarial Perturbation | Poster | [![Github](https://img.shields.io/github/stars/CityU-MLO/sPGD)](https://github.com/CityU-MLO/sPGD) | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/zhong24c.html) | - |
| Rethinking Adversarial Robustness in the Context of the Right to be Forgotten | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/zhao24k.html) | - |
| On the Duality Between Sharpness-Aware Minimization and Adversarial Training | Poster | [![Github](https://img.shields.io/github/stars/weizeming/SAM_AT)](https://github.com/weizeming/SAM_AT) | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/zhang24w.html) | - |
| Transferable Facial Privacy Protection against Blind Face Restoration via Domain-Consistent Adversarial Obfuscation | Poster | [![Github](https://img.shields.io/github/stars/CityU-MLO/sPGD)](https://github.com/CityU-MLO/sPGD) | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/zhang24co.html) | - |
| Improving Accuracy-robustness Trade-off via Pixel Reweighted Adversarial Training | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/zhang24am.html) | - |
| Manifold Integrated Gradients: Riemannian Geometry for Feature Attribution | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/zaher24a.html) | - |
| RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/yuan24f.html) | - |
| Improving Sharpness-Aware Minimization by Lookahead | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/yu24q.html) | - |
| Purify Unlearnable Examples via Rate-Constrained Variational Autoencoders | Poster | [![Github](https://img.shields.io/github/stars/yuyi-sd/D-VAE)](https://github.com/yuyi-sd/D-VAE) | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/yu24m.html) | - |
| Generalization Bound and New Algorithm for Clean-Label Backdoor Attack | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/yu24i.html) | - |
| Towards Robust Model-Based Reinforcement Learning Against Adversarial Corruption | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/ye24a.html) | - |
| Robust Universal Adversarial Perturbations | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/xu24v.html) | - |
| Uniformly Stable Algorithms for Adversarial Training and Beyond | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/xiao24e.html) | - |
| Delving into the Convergence of Generalized Smooth Minimax Optimization | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/xian24a.html) | - |
| Adversarially Robust Hypothesis Transfer Learning | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/wang24d.html) | - |
| Benign Overfitting in Adversarial Training of Neural Networks | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/wang24cn.html) | - |
| Collapse-Aware Triplet Decoupling for Adversarially Robust Image Retrieval | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/tian24a.html) | - |
| IOI: Invisible One-Iteration Adversarial Attack on No-Reference Image- and Video-Quality Metrics | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/shumitskaya24a.html) | - |
| Fast Adversarial Attacks on Language Models In One GPU Minute | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/sadasivan24a.html) | - |
| SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/park24h.html) | - |
| Two Heads are Actually Better than One: Towards Better Adversarial Robustness via Transduction and Rejection | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/palumbo24a.html) | - |
| Can Implicit Bias Imply Adversarial Robustness? | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/min24a.html) | - |
| The Pitfalls and Promise of Conformal Inference Under Adversarial Attacks | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/liu24m.html) | - |
| Enhancing Adversarial Robustness in SNNs with Sparse Gradients | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/liu24f.html) | - |
| OODRobustBench: a Benchmark and Large-Scale Analysis of Adversarial Robustness under Distribution Shift | Poster | [![Github](https://img.shields.io/github/stars/OODRobustBench/OODRobustBench)](https://github.com/OODRobustBench/OODRobustBench) | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/li24bp.html) | - |
| DataFreeShield: Defending Adversarial Attacks without Training Data | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/lee24f.html) | - |
| Adversarially Robust Deep Multi-View Clustering: A Novel Attack and Defense Framework | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/huang24ai.html) | - |
| Be Your Own Neighborhood: Detecting Adversarial Examples by the Neighborhood Relations Built on Self-Supervised Learning | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/he24l.html) | - |
| Consistent Adversarially Robust Linear Classification: Non-Parametric Setting | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/dohmatob24a.html) | - |
| Et Tu Certifications: Robustness Certificates Yield Better Adversarial Examples | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/cullen24a.html) | - |
| Efficient Black-box Adversarial Attacks via Bayesian Optimization Guided by a Function Prior | Poster | [![Github](https://img.shields.io/github/stars/yibo-miao/PBO-Attack)](https://github.com/yibo-miao/PBO-Attack) | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/cheng24h.html) | - |
| BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise Regression Tasks | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/cheng24e.html) | - |
| Robust Classification via a Single Diffusion Model | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/chen24k.html) | - |
| Adversarial Robustness Limits via Scaling-Law and Human-Alignment Studies | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/bartoldson24a.html) | - |
| Adversarial Attacks on Combinatorial Multi-Armed Bandits | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/balasubramanian24a.html) | - |
| Image Hijacks: Adversarial Images can Control Generative Models at Runtime | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/bailey24a.html) | - |
| Diffusion Models Demand Contrastive Guidance for Adversarial Purification to Advance | Poster | - | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/bai24b.html) | - |
| CosPGD: an efficient white-box adversarial attack for pixel-wise prediction tasks | Poster | [![Github](https://img.shields.io/github/stars/shashankskagnihotri/cospgd)](https://github.com/shashankskagnihotri/cospgd) | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/agnihotri24b.html) | - |
| Revisiting Character-level Adversarial Attacks for Language Models | Poster | [![Github](https://img.shields.io/github/stars/LIONS-EPFL/Charmer)](https://github.com/LIONS-EPFL/Charmer) | [![icml](https://img.shields.io/badge/pdf-PMLR-3333CC)](https://proceedings.mlr.press/v235/abad-rocamora24a.html) | - |
